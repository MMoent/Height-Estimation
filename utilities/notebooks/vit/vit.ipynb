{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81cddb2-6732-4e1b-a9e3-0026f392cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf1cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, patch_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.patch_num = (self.img_size // self.patch_size)**2\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)   # here use conv2d to replace cropping+fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b10baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "p = PatchEmbeddings(256, 3, 16, 16*16*3)\n",
    "x = torch.randn((4, 3, 256, 256))\n",
    "y = p(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6262e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, patch_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.patch_embedding = PatchEmbeddings(self.img_size, self.num_channels, self.patch_size, self.hidden_size)\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.hidden_size))\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.patch_embedding.patch_num+1, self.hidden_size))\n",
    "        self.dropout = nn.Dropout()\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        bs = x.shape[0]\n",
    "        cls_token = self.cls.expand(bs, -1, -1)\n",
    "        x = torch.concat([x, cls_token], dim=1)\n",
    "        x = x + self.position_embedding\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758e4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_head_size, bias=True) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attention_score = query @ key.permute(0, 2, 1)\n",
    "        attention_score = attention_score / math.sqrt(self.attention_head_size)\n",
    "        attention_score = nn.functional.softmax(attention_score, dim=-1)\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        attention_map = attention_score @ value\n",
    "        return attention_map\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb815b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = AttentionHead(768, 64)\n",
    "x = torch.randn(4, 257, 768)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c7184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.total_head_size = self.attention_head_size * self.num_attention_heads\n",
    "\n",
    "        self.heads = nn.ModuleList()\n",
    "        [self.heads.append(AttentionHead(self.hidden_size, self.attention_head_size)) for _ in range(self.num_attention_heads)]\n",
    "\n",
    "        self.output_projection = nn.Linear(self.total_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat(attention_outputs, dim=-1)\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bcb01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MultiHeadAttention(768, 12)\n",
    "x = torch.randn(4, 257, 768)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ec002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, middle_size) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.middle_size = middle_size\n",
    "        self.dense1 = nn.Linear(self.hidden_size, self.middle_size)\n",
    "        self.dense2 = nn.Linear(self.middle_size, self.hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1783f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "mlp = MLP(768, 768)\n",
    "y = mlp(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7695eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden1, hidden2) -> None:\n",
    "        super().__init__()\n",
    "        self.attention1 = MultiHeadAttention(hidden1, 12)\n",
    "        self.laynorm1 = nn.LayerNorm(hidden1)\n",
    "        self.attention2 = MultiHeadAttention(hidden2, 12)\n",
    "        self.laynorm2 = nn.LayerNorm(hidden2)\n",
    "        self.mlp = MLP(hidden2, hidden2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention1(self.laynorm1(x))\n",
    "        x = x + attention_output\n",
    "        mlp_output = self.mlp(self.laynorm2(x))\n",
    "        x = x + mlp_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3067591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "b = Block(768, 768)\n",
    "y = b(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8edffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden1, hidden2, block_num) -> None:\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        [self.blocks.append(Block(hidden1, hidden2)) for _ in range(block_num)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e108421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, patch_size, hidden_size, block_num, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.block_num = block_num\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = Embeddings(self.img_size, self.num_channels, self.patch_size, self.hidden_size)\n",
    "        self.encoder = Encoder(self.hidden_size, self.hidden_size, self.block_num)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        encoder_output = self.encoder(embedding)\n",
    "        logits = self.classifier(encoder_output[:, 0])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e648c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_param(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c81ec6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24432394\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 3, 256, 256)\n",
    "vit = ViT(img_size=256, num_channels=3, patch_size=16, hidden_size=768, block_num=4, num_classes=10)\n",
    "y = vit(x)\n",
    "print(count_model_param(vit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
