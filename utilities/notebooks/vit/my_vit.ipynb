{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, patch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_num = (img_size // patch_size)**2\n",
    "\n",
    "        self.proj = nn.Conv2d(self.num_channels, self.num_channels*self.patch_size**2, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.positional_embedding = self.get_positional_embedding()\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, self.num_channels*self.patch_size**2))\n",
    "\n",
    "    def get_positional_embedding(self, n=10000):\n",
    "        d_model = self.num_channels*self.patch_size**2\n",
    "        seq_len = self.patch_num + 1\n",
    "        even_i = torch.arange(0, d_model, 2).float()\n",
    "        odd_i = torch.arange(1, d_model, 2).float()\n",
    "        even_denominator = torch.pow(n, even_i/d_model)\n",
    "        odd_denominator = torch.pow(n, odd_i/d_model)\n",
    "        position = torch.arange(seq_len, dtype=torch.float).reshape(-1, 1)\n",
    "        even_pe = torch.sin(position/even_denominator)\n",
    "        odd_pe = torch.cos(position/odd_denominator)\n",
    "        positional_embedding = torch.stack([even_pe, odd_pe], dim=2).flatten(1)\n",
    "        return positional_embedding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        cls = self.cls.expand(bs, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        positional_embedding = self.positional_embedding.expand(bs, -1, -1)\n",
    "        x = x + positional_embedding\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Embedding(256, 3, 16)\n",
    "x = torch.randn(4, 3, 256, 256)\n",
    "y = embedding(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout_p=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.W_q = nn.Linear(self.dim_in, self.dim_out)\n",
    "        self.W_k = nn.Linear(self.dim_in, self.dim_out)\n",
    "        self.W_v = nn.Linear(self.dim_in, self.dim_out)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_q(x)\n",
    "        key = self.W_k(x)\n",
    "        value = self.W_v(x)\n",
    "        # scaled dot-product\n",
    "        attention_score = nn.functional.softmax(query @ key.transpose(1, 2) / math.sqrt(self.dim_out), dim=-1)\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        attention_output = attention_score @ value\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "a = AttentionHead(768, 64)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, att_head_dim_in, att_head_dim_out, num_head, dropout_p=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.att_head_dim_in = att_head_dim_in\n",
    "        self.att_head_dim_out = att_head_dim_out\n",
    "        self.num_head = num_head\n",
    "        self.total_dim_out = num_head * att_head_dim_out\n",
    "\n",
    "        self.heads = nn.ModuleList()\n",
    "        [self.heads.append(AttentionHead(self.att_head_dim_in, self.att_head_dim_out)) for _ in range(self.num_head)]\n",
    "        self.proj = nn.Linear(self.total_dim_out, self.total_dim_out)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "a = MultiheadAttention(768, 64, 12)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_mid, dim_out, dropout_p=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_mid = dim_mid\n",
    "        self.dim_out = dim_out\n",
    "        self.dense1 = nn.Linear(self.dim_in, self.dim_mid)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dense2 = nn.Linear(self.dim_mid, self.dim_out)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "a = MLP(768, 768, 768)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_head) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(self.dim_in)\n",
    "        self.multihead_attention = MultiheadAttention(self.dim_in, self.dim_in//self.num_head, self.num_head)\n",
    "        self.layernorm2 = nn.LayerNorm(self.dim_in)\n",
    "        self.mlp = MLP(self.dim_in, self.dim_in, self.dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multihead_attention(self.layernorm1(x))\n",
    "        x = x + self.mlp(self.layernorm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 257, 768)\n",
    "a = TransformerBlock(768, 768, 12)\n",
    "y = a(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, num_channels, patch_size, num_blocks, num_heads, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_size = num_channels*patch_size**2\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = Embedding(self.img_size, self.num_channels, self.patch_size)\n",
    "        self.encoder = nn.Sequential()\n",
    "        [self.encoder.append(TransformerBlock(self.hidden_size, self.hidden_size, self.num_heads)) for _ in range(self.num_blocks)]\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 3, 256, 256)\n",
    "vit = ViT(img_size=256, num_channels=3, patch_size=16, num_heads=12, num_blocks=4, num_classes=10)\n",
    "y = vit(x)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
